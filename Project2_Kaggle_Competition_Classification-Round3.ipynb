{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "### <font style=\"color:blue\">Project 2: Kaggle Competition - Classification</font>\n",
    "\n",
    "#### Maximum Points: 100\n",
    "\n",
    "<div>\n",
    "    <table>\n",
    "        <tr><td><h3>Sr. no.</h3></td> <td><h3>Section</h3></td> <td><h3>Points</h3></td> </tr>\n",
    "        <tr><td><h3>1</h3></td> <td><h3>Data Loader</h3></td> <td><h3>10</h3></td> </tr>\n",
    "        <tr><td><h3>2</h3></td> <td><h3>Configuration</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>3</h3></td> <td><h3>Evaluation Metric</h3></td> <td><h3>10</h3></td> </tr>\n",
    "        <tr><td><h3>4</h3></td> <td><h3>Train and Validation</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>5</h3></td> <td><h3>Model</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>6</h3></td> <td><h3>Utils</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>7</h3></td> <td><h3>Experiment</h3></td><td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>8</h3></td> <td><h3>TensorBoard Dev Scalars Log Link</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>9</h3></td> <td><h3>Kaggle Profile Link</h3></td> <td><h3>50</h3></td> </tr>\n",
    "    </table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">1. Data Loader [10 Points]</font>\n",
    "\n",
    "In this section, you have to write a class or methods, which will be used to get training and validation data loader.\n",
    "\n",
    "You need to write a custom dataset class to load data.\n",
    "\n",
    "**Note; There is   no separate validation data. , You will thus have to create your own validation set, by dividing the train data into train and validation data. Usually, we do 80:20 ratio for train and validation, respectively.**\n",
    "\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "class KenyanFood13Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "    ....\n",
    "    ...\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "    ...\n",
    "    ...\n",
    "    \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "def get_data(args1, *args):\n",
    "    ....\n",
    "    ....\n",
    "    return train_loader, test_loader\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "from typing import Iterable\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Custom Data Loader and Train/Validation Split\n",
    "\n",
    "# We will implement a custom Dataset class that reads image paths and labels \n",
    "# from `train.csv`. The dataset will be split into training and validation \n",
    "# sets (80/20) using a fixed random seed for reproducibility. DataLoader \n",
    "# objects will be created for both splits.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class KenyanFood13Dataset(Dataset):\n",
    "    def __init__(self, csv_file, data_dir, transform=None, class_to_idx=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        # Build class_to_idx if not provided\n",
    "        if class_to_idx is None:\n",
    "            classes = sorted(self.data.iloc[:, 1].unique())\n",
    "            self.class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "        else:\n",
    "            self.class_to_idx = class_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = str(self.data.iloc[idx, 0])\n",
    "        if not img_name.lower().endswith('.jpg'):\n",
    "            img_name += '.jpg'\n",
    "        label_str = self.data.iloc[idx, 1]\n",
    "        label = self.class_to_idx[label_str]\n",
    "        img_path = os.path.join(self.data_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def trasformations(data_augmentation=True):\n",
    "    \n",
    "    common_transform = transforms.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),  # still, we need normalization\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    augmentations = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
    "        transforms.RandomGrayscale(p=0.05),\n",
    "        transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.05),\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    if data_augmentation:\n",
    "        train_transform = transforms.Compose([\n",
    "            augmentations,\n",
    "            common_transform\n",
    "        ])\n",
    "    else:\n",
    "        train_transform = common_transform\n",
    "        \n",
    "    return train_transform, common_transform\n",
    "    \n",
    "    \n",
    "def data_loader_factory(csv_file, img_dir, \n",
    "                        train_transform, val_transform, \n",
    "                        train_idx, val_idx, \n",
    "                        batch_size=8,  \n",
    "                        num_workers=2,\n",
    "                        class_to_idx=None):\n",
    "    # Datasets with transforms\n",
    "    train_dataset = KenyanFood13Dataset(csv_file, img_dir, transform=train_transform, class_to_idx=class_to_idx)\n",
    "    val_dataset = KenyanFood13Dataset(csv_file, img_dir, transform=val_transform, class_to_idx=class_to_idx)\n",
    "    # Slice datasets using indices\n",
    "    train_subset = Subset(train_dataset, train_idx)\n",
    "    val_subset = Subset(val_dataset, val_idx)\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "\n",
    "def index_split(csv_file, img_dir, random_state=10):\n",
    "    \n",
    "    full_dataset = KenyanFood13Dataset(csv_file, img_dir, transform=None)\n",
    "    \n",
    "    indices = list(range(len(full_dataset)))\n",
    "    \n",
    "    train_idx, val_idx = train_test_split(\n",
    "        indices, test_size=0.2, random_state=random_state, stratify=full_dataset.data.iloc[:,1]\n",
    "    )\n",
    "    \n",
    "    return train_idx, val_idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_data(batch_size=8, \n",
    "             data_root='./data', \n",
    "             num_workers=2, \n",
    "             seed=10, \n",
    "             data_augmentation=True):\n",
    "    \n",
    "    num_workers = 0\n",
    "    \n",
    "    # load label file and image directory\n",
    "    csv_file = os.path.join(data_root, 'train.csv')\n",
    "    img_dir = os.path.join(data_root, 'images/images')\n",
    "\n",
    "    # Build class_to_idx mapping\n",
    "    df = pd.read_csv(csv_file)\n",
    "    classes = sorted(df.iloc[:, 1].unique())\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "\n",
    "    # get transformations\n",
    "    train_transform, val_transform = trasformations(data_augmentation)\n",
    "\n",
    "    # get train and validation indices\n",
    "    train_idx, val_idx = index_split(\n",
    "        csv_file=csv_file,\n",
    "        img_dir=img_dir,\n",
    "        random_state=seed\n",
    "    )\n",
    "\n",
    "    # create dataloaders\n",
    "    train_loader, val_loader = data_loader_factory(\n",
    "        csv_file=csv_file,\n",
    "        img_dir=img_dir,\n",
    "        train_transform=train_transform,\n",
    "        val_transform=val_transform,\n",
    "        train_idx=train_idx,\n",
    "        val_idx=val_idx,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        class_to_idx=class_to_idx\n",
    "    )\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "trusted": true,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def image_preprocess_transforms():\n",
    "    \n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "    \n",
    "    return preprocess\n",
    "\n",
    "\n",
    "def image_common_transforms(mean=(0.4611, 0.4359, 0.3905), std=(0.2193, 0.2150, 0.2109)):\n",
    "    preprocess = image_preprocess_transforms()\n",
    "    \n",
    "    common_transforms = transforms.Compose([\n",
    "        preprocess,\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    \n",
    "    return common_transforms\n",
    "\n",
    "\n",
    "def data_loader(data_root, transform, batch_size=16, shuffle=False, num_workers=2):\n",
    "    dataset = datasets.ImageFolder(root=data_root, transform=transform)\n",
    "    \n",
    "    loader = torch.utils.data.DataLoader(dataset, \n",
    "                                         batch_size=batch_size,\n",
    "                                         num_workers=num_workers,\n",
    "                                         shuffle=shuffle)\n",
    "    \n",
    "    return loader\n",
    "\n",
    "def get_mean_std(data_root, num_workers=4):\n",
    "    \n",
    "    transform = image_preprocess_transforms()\n",
    "    \n",
    "    loader = data_loader(data_root, transform)\n",
    "    \n",
    "    batch_mean = torch.zeros(3)\n",
    "    batch_mean_sqrd = torch.zeros(3)\n",
    "    \n",
    "    for batch_data, _ in loader:\n",
    "        batch_mean += batch_data.mean(dim=(0, 2, 3)) # E[batch_i] \n",
    "        batch_mean_sqrd += (batch_data ** 2).mean(dim=(0, 2, 3)) #  E[batch_i**2]\n",
    "    \n",
    "    # E[dataset] = E[E[batch_1], E[batch_2], ...]\n",
    "    mean = batch_mean / len(loader)\n",
    "    \n",
    "    # var[X] = E[X**2] - E[X]**2\n",
    "    \n",
    "    # E[X**2] = E[E[batch_1**2], E[batch_2**2], ...]\n",
    "    # E[X]**2 = E[E[batch_1], E[batch_2], ...] ** 2\n",
    "    \n",
    "    var = (batch_mean_sqrd / len(loader)) - (mean ** 2)\n",
    "        \n",
    "    std = var ** 0.5\n",
    "    print('mean: {}, std: {}'.format(mean, std))\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "\n",
    "#def get_data(batch_size, data_root, num_workers=4, data_augmentation=False):\n",
    "#     YOUR CODE HERE\n",
    "\n",
    "def data_augmentation_preprocess(mean, std):\n",
    "    \n",
    "    augmentations = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
    "        transforms.RandomGrayscale(p=0.05),\n",
    "        transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.05),\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    augmentations =  transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.05),\n",
    "        transforms.RandomGrayscale(p=0.05),\n",
    "        transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.1),\n",
    "    ])\n",
    "    \n",
    "    augmentations = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),     # 🔼 broader scale\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(20),                           # 🔼 more rotation\n",
    "        transforms.ColorJitter(brightness=0.45, contrast=0.45, saturation=0.45, hue=0.1),               # 🔼 more color variation\n",
    "        transforms.RandomGrayscale(p=0.1),                       # 🔼 more aggressive\n",
    "        transforms.RandomApply([transforms.GaussianBlur(kernel_size=5)], p=0.2),  # 🔼 more blur, higher prob\n",
    "    ])\n",
    "    \n",
    "\n",
    "    preprocess = image_preprocess_transforms()  # Resize → CenterCrop → ToTensor\n",
    "\n",
    "    return transforms.Compose([\n",
    "        augmentations,\n",
    "        preprocess,\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_data(batch_size, data_root, num_workers=4, data_augmentation=False):\n",
    "    \n",
    "    train_data_path = os.path.join(data_root, 'training')\n",
    "       \n",
    "    mean, std = get_mean_std(data_root=train_data_path, num_workers=num_workers)\n",
    "    \n",
    "    common_transforms = image_common_transforms(mean, std)\n",
    "        \n",
    "   \n",
    "    # if data_augmentation is true \n",
    "    # data augmentation implementation\n",
    "    if data_augmentation:    \n",
    "        train_transforms = data_augmentation_preprocess(mean, std)\n",
    "    # else do common transforms\n",
    "    else:\n",
    "        train_transforms = common_transforms\n",
    "        \n",
    "        \n",
    "    # train dataloader\n",
    "    \n",
    "    train_loader = data_loader(train_data_path, \n",
    "                               train_transforms, \n",
    "                               batch_size=batch_size, \n",
    "                               shuffle=True, \n",
    "                               num_workers=num_workers)\n",
    "    \n",
    "    # test dataloader\n",
    "    \n",
    "    test_data_path = os.path.join(data_root, 'validation')\n",
    "    \n",
    "    test_loader = data_loader(test_data_path, \n",
    "                              common_transforms, #train_transforms, \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=False, \n",
    "                              num_workers=num_workers)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2. Configuration [5 Points]</font>\n",
    "\n",
    "**Define your configuration here.**\n",
    "\n",
    "For example:\n",
    "\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class TrainingConfiguration:\n",
    "    '''\n",
    "    Describes configuration of the training process\n",
    "    '''\n",
    "    batch_size: int = 10 \n",
    "    epochs_count: int = 50  \n",
    "    init_learning_rate: float = 0.1  # initial learning rate for lr scheduler\n",
    "    log_interval: int = 5  \n",
    "    test_interval: int = 1  \n",
    "    data_root: str = \"/kaggle/input/opencv-pytorch-project-2-classification-round-3\" \n",
    "    num_workers: int = 2  \n",
    "    device: str = 'cuda'  \n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SystemConfiguration:\n",
    "    '''\n",
    "    Describes the common system setting needed for reproducible training\n",
    "    '''\n",
    "    seed: int = 10  # seed number to set the state of all random number generators\n",
    "    cudnn_benchmark_enabled: bool = True  # enable CuDNN benchmark for the sake of performance\n",
    "    cudnn_deterministic: bool = True  # make cudnn deterministic (reproducible training)\n",
    "    \n",
    "    \n",
    "def setup_system(system_config: SystemConfiguration) -> None:\n",
    "    torch.manual_seed(system_config.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n",
    "        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfiguration:\n",
    "    '''\n",
    "    Describes configuration of the training process\n",
    "    '''\n",
    "    batch_size: int = 4 \n",
    "    epochs_count: int = 350  \n",
    "    init_learning_rate: float = 1e-3 #5e-4 #4e-06 #0.1  # initial learning rate for lr scheduler\n",
    "    weight_decay: float = 2e-4 #1e-4 #5e-5 #5e-6  # weight decay for optimizer\n",
    "    log_interval: int = 5  \n",
    "    test_interval: int = 1  \n",
    "    data_root: str = \"./data\" \n",
    "    num_workers: int = 2  \n",
    "    device: str = 'cuda' #'cuda'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">3. Evaluation Metric [10 Points]</font>\n",
    "\n",
    "**Define methods or classes that will be used in model evaluation. For example, accuracy, f1-score etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">4. Train and Validation [5 Points]</font>\n",
    "\n",
    "\n",
    "**Write the methods or classes to be used for training and validation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_config: TrainingConfiguration, \n",
    "    model: nn.Module, \n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    train_loader: torch.utils.data.DataLoader, \n",
    "    epoch_idx: int\n",
    ") -> None:\n",
    "    \n",
    "    # change model in training mood\n",
    "    model.train()\n",
    "    \n",
    "    # to get batch loss\n",
    "    batch_loss = np.array([])\n",
    "    \n",
    "    # to get batch accuracy\n",
    "    batch_acc = np.array([])\n",
    "        \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        # clone target\n",
    "        indx_target = target.clone()\n",
    "        # send data to device (its is medatory if GPU has to be used)\n",
    "        data = data.to(train_config.device)\n",
    "        # send target to device\n",
    "        target = target.to(train_config.device)\n",
    "\n",
    "        # reset parameters gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass to the model\n",
    "        output = model(data)\n",
    "        \n",
    "        # cross entropy loss\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        \n",
    "        # find gradients w.r.t training parameters\n",
    "        loss.backward()\n",
    "        # Update parameters using gardients\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss = np.append(batch_loss, [loss.item()])\n",
    "        \n",
    "        # Score to probability using softmax\n",
    "        prob = F.softmax(output, dim=1)\n",
    "            \n",
    "        # get the index of the max probability\n",
    "        pred = prob.data.max(dim=1)[1]  \n",
    "                        \n",
    "        # correct prediction\n",
    "        correct = pred.cpu().eq(indx_target).sum()\n",
    "            \n",
    "        # accuracy\n",
    "        acc = float(correct) / float(len(data))\n",
    "        \n",
    "        batch_acc = np.append(batch_acc, [acc])\n",
    "            \n",
    "    epoch_loss = batch_loss.mean()\n",
    "    epoch_acc = batch_acc.mean()\n",
    "    print('Epoch: {} \\nTrain Loss: {:.6f} Acc: {:.4f}'.format(epoch_idx, epoch_loss, epoch_acc))\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "\n",
    "def validate(\n",
    "    train_config: TrainingConfiguration,\n",
    "    model: nn.Module,\n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    ") -> float:\n",
    "    # \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    count_corect_predictions = 0\n",
    "    for data, target in test_loader:\n",
    "        indx_target = target.clone()\n",
    "        data = data.to(train_config.device)\n",
    "        \n",
    "        target = target.to(train_config.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(data)\n",
    "        \n",
    "        # add loss for each mini batch\n",
    "        test_loss += F.cross_entropy(output, target).item()\n",
    "        \n",
    "        # Score to probability using softmax\n",
    "        prob = F.softmax(output, dim=1)\n",
    "        \n",
    "        # get the index of the max probability\n",
    "        pred = prob.data.max(dim=1)[1] \n",
    "        \n",
    "        # add correct prediction count\n",
    "        count_corect_predictions += pred.cpu().eq(indx_target).sum()\n",
    "\n",
    "    # average over number of mini-batches\n",
    "    test_loss = test_loss / len(test_loader)  \n",
    "    \n",
    "    # average over number of dataset\n",
    "    accuracy = 100. * count_corect_predictions / len(test_loader.dataset)\n",
    "    \n",
    "    print(\n",
    "        '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, count_corect_predictions, len(test_loader.dataset), accuracy\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return test_loss, accuracy/100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">5. Model [5 Points]</font>\n",
    "\n",
    "**Define your model in this section.**\n",
    "\n",
    "**You are allowed to use any pre-trained model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "#     YOUR CODE HERE\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # # convolution layers\n",
    "        # self._body = nn.Sequential(\n",
    "            \n",
    "        #     nn.Conv2d(in_channels=3, out_channels=32, kernel_size=7),\n",
    "        #     nn.BatchNorm2d(32),\n",
    "        #     nn.ReLU(inplace=True),\n",
    "        #     nn.MaxPool2d(kernel_size=2),\n",
    "                        \n",
    "        #     nn.Conv2d(in_channels=32, out_channels=128, kernel_size=5),\n",
    "        #     nn.BatchNorm2d(128),\n",
    "        #     nn.ReLU(inplace=True),\n",
    "        #     nn.MaxPool2d(kernel_size=4),\n",
    "                        \n",
    "        # )\n",
    "        \n",
    "        self._body = nn.Sequential(\n",
    "            # Block 1: large kernel\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=7, stride=2, padding=3),  # 224 → 112\n",
    "            nn.BatchNorm2d(32),\n",
    "            #nn.ReLU(inplace=True),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "\n",
    "\n",
    "            # Block 2: medium kernel\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=2, padding=2),  # 112 → 56\n",
    "            nn.BatchNorm2d(64),\n",
    "            #nn.ReLU(inplace=True),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            \n",
    "            # Block 3: standard kernel\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1),  # 56 → 28\n",
    "            nn.BatchNorm2d(128),\n",
    "            #nn.ReLU(inplace=True),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "                        \n",
    "            # Block 4\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1), # 28 → 14\n",
    "            nn.BatchNorm2d(256),\n",
    "            #nn.ReLU(inplace=True),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "                        \n",
    "            #nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            # Block 5\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1), # 14 → 7\n",
    "            nn.BatchNorm2d(256),\n",
    "            #nn.ReLU(inplace=True),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            \n",
    "            # one touch of dropout\n",
    "            nn.Dropout2d(p=0.1)  # or 0.15\n",
    "\n",
    "            \n",
    "            )\n",
    "\n",
    "        self._body = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),  # 224 → 112\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),  # 112 → 56\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "\n",
    "            # Block 4\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),  # 56 → 28\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "\n",
    "            # Block 5\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "\n",
    "            # Block 6\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),  # 28 → 14\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "\n",
    "            # Block 7\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "\n",
    "            # Block 8\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),  # 14 → 7\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "\n",
    "            nn.Dropout2d(p=0.2),\n",
    "\n",
    "            # Global Average Pooling → [B, 512, 1, 1]\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Fully connected layers\n",
    "        self._head = nn.Sequential( \n",
    "            nn.Linear(in_features=256*7*7, out_features=1024), \n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.Dropout(p=0.4),  # Dropout after activation\n",
    "            \n",
    "            nn.Linear(in_features=1024, out_features=512),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.Dropout(p=0.4),  # Dropout after activation\n",
    "            \n",
    "            nn.Linear(in_features=512, out_features=128),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.Dropout(p=0.4),  # Dropout after activation\n",
    "\n",
    "            nn.Linear(in_features=128, out_features=3),\n",
    "        )\n",
    "    \n",
    "    \n",
    "        self._head = nn.Sequential(\n",
    "            nn.Flatten(),  # [B, 512]\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LayerNorm(1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=0.4),\n",
    "\n",
    "            nn.Linear(512, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "\n",
    "            nn.Linear(128, 3)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # apply feature extractor\n",
    "        x = self._body(x)\n",
    "        # flatten the output of conv layers\n",
    "        # dimension should be batch_size * number_of weight_in_last conv_layer\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        # apply classification head\n",
    "        x = self._head(x)\n",
    "        \n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main(model, optimizer, scheduler=None, system_configuration=SystemConfiguration(), \n",
    "         training_configuration=TrainingConfiguration(), data_augmentation=True):\n",
    "    \n",
    "    # system configuration\n",
    "    setup_system(system_configuration)\n",
    "\n",
    "    # batch size\n",
    "    batch_size_to_set = training_configuration.batch_size\n",
    "    # num_workers\n",
    "    num_workers_to_set = training_configuration.num_workers\n",
    "    # epochs\n",
    "    epoch_num_to_set = training_configuration.epochs_count\n",
    "\n",
    "    # if GPU is available use training config, \n",
    "    # else lowers batch_size, num_workers and epochs count\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Cuda is Available!!!\")\n",
    "        device = \"cuda\"\n",
    "    #elif torch.backends.mps.is_available():\n",
    "    #    device = \"mps\"\n",
    "    #    batch_size_to_set = 8\n",
    "    else:\n",
    "        raise RuntimeError(\"No GPU - check your setup.\")\n",
    "        device = \"cpu\"\n",
    "        batch_size_to_set = 16\n",
    "        num_workers_to_set = 4\n",
    "\n",
    "    # data loader\n",
    "    train_loader, test_loader = get_data(\n",
    "        batch_size=batch_size_to_set,\n",
    "        data_root=training_configuration.data_root,\n",
    "        num_workers=num_workers_to_set,\n",
    "        data_augmentation=data_augmentation\n",
    "    )\n",
    "    \n",
    "    # Update training configuration\n",
    "    training_configuration = TrainingConfiguration(\n",
    "        device=device,\n",
    "        batch_size=batch_size_to_set,\n",
    "        num_workers=num_workers_to_set\n",
    "    )\n",
    "        \n",
    "    # send model to device (GPU/CPU)\n",
    "    model.to(training_configuration.device)\n",
    "\n",
    "    best_loss = torch.tensor(np.inf)\n",
    "    \n",
    "    # epoch train/test loss\n",
    "    epoch_train_loss = np.array([])\n",
    "    epoch_test_loss = np.array([])\n",
    "    \n",
    "    # epch train/test accuracy\n",
    "    epoch_train_acc = np.array([])\n",
    "    epoch_test_acc = np.array([])\n",
    "    \n",
    "    # Calculate Initial Test Loss\n",
    "    init_val_loss, init_val_accuracy = validate(training_configuration, model, test_loader)\n",
    "    print(\"Initial Test Loss : {:.6f}, \\nInitial Test Accuracy : {:.3f}%\\n\".format(init_val_loss, \n",
    "                                                                                   init_val_accuracy*100))\n",
    "    \n",
    "    # trainig time measurement\n",
    "    t_begin = time.time()\n",
    "    for epoch in range(training_configuration.epochs_count):\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train(training_configuration, model, optimizer, train_loader, epoch)\n",
    "        \n",
    "        epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n",
    "        \n",
    "        epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n",
    "\n",
    "        elapsed_time = time.time() - t_begin\n",
    "        speed_epoch = elapsed_time / (epoch + 1)\n",
    "        speed_batch = speed_epoch / len(train_loader)\n",
    "        eta = speed_epoch * training_configuration.epochs_count - elapsed_time\n",
    "        \n",
    "        print(\n",
    "            \"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\n",
    "                elapsed_time, speed_epoch, speed_batch, eta\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Validate\n",
    "        if epoch % training_configuration.test_interval == 0:\n",
    "            current_loss, current_accuracy = validate(training_configuration, model, test_loader)\n",
    "            \n",
    "            epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n",
    "        \n",
    "            epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n",
    "            \n",
    "            \n",
    "            # 🔽 Step the scheduler with validation loss\n",
    "            if scheduler is not None:\n",
    "                #scheduler.step(current_loss)\n",
    "                scheduler.step()\n",
    "                # print the new learning rate    \n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(f\"Current learning rate: {param_group['lr']}\\n\")\n",
    "            \n",
    "            \n",
    "            if current_loss < best_loss:\n",
    "                best_loss = current_loss\n",
    "                print('Model Improved. Saving the Model...\\n')\n",
    "                save_model(model, device=training_configuration.device)\n",
    "        \n",
    "                \n",
    "    print(\"Total time: {:.2f}, Best Loss: {:.3f}\".format(time.time() - t_begin, best_loss))\n",
    "    \n",
    "    return model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">6. Utils [5 Points]</font>\n",
    "\n",
    "**Define those methods or classes, which have  not been covered in the above sections.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, device, model_dir='models', model_file_name='cat_dog_panda_classifier.pt'):\n",
    "    \n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "\n",
    "    model_path = os.path.join(model_dir, model_file_name)\n",
    "\n",
    "    # make sure you transfer the model to cpu.\n",
    "    if device == 'cuda':\n",
    "        model.to('cpu')\n",
    "\n",
    "    # save the state_dict\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    if device == 'cuda':\n",
    "        model.to('cuda')\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def load_model(model, model_dir='models', model_file_name='cat_dog_panda_classifier.pt'):\n",
    "    model_path = os.path.join(model_dir, model_file_name)\n",
    "\n",
    "    # loading the model and getting model parameters by using load_state_dict\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">7. Experiment [5 Points]</font>\n",
    "\n",
    "**Choose your optimizer and LR-scheduler and use the above methods and classes to train your model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (_body): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (3): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (22): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (23): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (24): Dropout2d(p=0.2, inplace=False)\n",
      "    (25): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      "  (_head): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): GELU(approximate='none')\n",
      "    (4): Dropout(p=0.5, inplace=False)\n",
      "    (5): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (6): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (7): GELU(approximate='none')\n",
      "    (8): Dropout(p=0.4, inplace=False)\n",
      "    (9): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (10): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (11): GELU(approximate='none')\n",
      "    (12): Dropout(p=0.3, inplace=False)\n",
      "    (13): Linear(in_features=128, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MyModel()\n",
    "print(model)\n",
    "\n",
    "# get optimizer\n",
    "train_config = TrainingConfiguration()\n",
    "\n",
    "### CHANGE HERE ###\n",
    "\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr = train_config.init_learning_rate,\n",
    "    weight_decay=train_config.weight_decay\n",
    ")\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer,\n",
    "#     mode='min',          # or 'max' if you're monitoring accuracy\n",
    "#     factor=0.75,          # shrink LR by half\n",
    "#     patience=5,          # wait 5 epochs before reducing\n",
    "#     min_lr=1e-6,         # don’t go too low\n",
    "# )\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=6,\n",
    "    gamma=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(train_loss, val_loss, train_acc, val_acc, colors, \n",
    "                       loss_legend_loc='upper center', acc_legend_loc='upper left', \n",
    "                       fig_size=(20, 10), sub_plot1=(1, 2, 1), sub_plot2=(1, 2, 2)):\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    plt.subplot(sub_plot1[0], sub_plot1[1], sub_plot1[2])\n",
    "    \n",
    "    for i in range(len(train_loss)):\n",
    "        x_train = range(len(train_loss[i]))\n",
    "        x_val = range(len(val_loss[i]))\n",
    "        \n",
    "        min_train_loss = train_loss[i].min()\n",
    "        \n",
    "        min_val_loss = val_loss[i].min()\n",
    "        \n",
    "        plt.plot(x_train, train_loss[i], linestyle='-', color='tab:{}'.format(colors[i]), \n",
    "                 label=\"TRAIN LOSS ({0:.4})\".format(min_train_loss))\n",
    "        plt.plot(x_val, val_loss[i], linestyle='--' , color='tab:{}'.format(colors[i]), \n",
    "                 label=\"VALID LOSS ({0:.4})\".format(min_val_loss))\n",
    "        \n",
    "    plt.xlabel('epoch no.')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc=loss_legend_loc)\n",
    "    plt.title('Training and Validation Loss')\n",
    "        \n",
    "    plt.subplot(sub_plot2[0], sub_plot2[1], sub_plot2[2])\n",
    "    \n",
    "    for i in range(len(train_acc)):\n",
    "        x_train = range(len(train_acc[i]))\n",
    "        x_val = range(len(val_acc[i]))\n",
    "        \n",
    "        max_train_acc = train_acc[i].max() \n",
    "        \n",
    "        max_val_acc = val_acc[i].max() \n",
    "        \n",
    "        plt.plot(x_train, train_acc[i], linestyle='-', color='tab:{}'.format(colors[i]), \n",
    "                 label=\"TRAIN ACC ({0:.4})\".format(max_train_acc))\n",
    "        plt.plot(x_val, val_acc[i], linestyle='--' , color='tab:{}'.format(colors[i]), \n",
    "                 label=\"VALID ACC ({0:.4})\".format(max_val_acc))\n",
    "        \n",
    "    plt.xlabel('epoch no.')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(loc=acc_legend_loc)\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    \n",
    "    fig.savefig('sample_loss_acc_plot.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is Available!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:245: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.\n"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# train and validate\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model, train_loss, train_acc, val_loss, val_acc = \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                                                       \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                                                       \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                                                       \u001b[49m\u001b[43mdata_augmentation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m plot_loss_accuracy(train_loss=[train_loss], \n\u001b[32m      9\u001b[39m                    val_loss=[val_loss], \n\u001b[32m     10\u001b[39m                    train_acc=[train_acc], \n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m                    loss_legend_loc=\u001b[33m'\u001b[39m\u001b[33mupper center\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     14\u001b[39m                    acc_legend_loc=\u001b[33m'\u001b[39m\u001b[33mupper left\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(model, optimizer, scheduler, system_configuration, training_configuration, data_augmentation)\u001b[39m\n\u001b[32m     53\u001b[39m epoch_test_acc = np.array([])\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Calculate Initial Test Loss\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m init_val_loss, init_val_accuracy = \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_configuration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInitial Test Loss : \u001b[39m\u001b[38;5;132;01m{:.6f}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mInitial Test Accuracy : \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.format(init_val_loss, \n\u001b[32m     58\u001b[39m                                                                                init_val_accuracy*\u001b[32m100\u001b[39m))\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# trainig time measurement\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 83\u001b[39m, in \u001b[36mvalidate\u001b[39m\u001b[34m(train_config, model, test_loader)\u001b[39m\n\u001b[32m     80\u001b[39m     output = model(data)\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# add loss for each mini batch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m test_loss += \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Score to probability using softmax\u001b[39;00m\n\u001b[32m     86\u001b[39m prob = F.softmax(output, dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# train and validate\n",
    "model, train_loss, train_acc, val_loss, val_acc = main(model, \n",
    "                                                       optimizer, \n",
    "                                                       scheduler=scheduler, \n",
    "                                                       data_augmentation=True)\n",
    "\n",
    "\n",
    "plot_loss_accuracy(train_loss=[train_loss], \n",
    "                   val_loss=[val_loss], \n",
    "                   train_acc=[train_acc], \n",
    "                   val_acc=[val_acc], \n",
    "                   colors=['blue'], \n",
    "                   loss_legend_loc='upper center', \n",
    "                   acc_legend_loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">8. TensorBoard Log Link [5 Points]</font>\n",
    "\n",
    "**Share your TensorBoard scalars logs link here You can also share (not mandatory) your GitHub link, if you have pushed this project in GitHub.**\n",
    "\n",
    "\n",
    "Note: In light of the recent shutdown of tensorboard.dev, we have updated the submission requirements for your project. Instead of sharing a tensorboard.dev link, you are now required to upload your generated TensorBoard event files directly onto the lab. As an alternative, you may also include a screenshot of your TensorBoard output within your Jupyter notebook. This adjustment ensures that your data visualization and model training efforts are thoroughly documented and accessible for evaluation.\n",
    "\n",
    "You are also welcome (and encouraged) to utilize alternative logging services like wandB or comet. In such instances, you can easily make your project logs publicly accessible and share the link with others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">9. Kaggle Profile Link [50 Points]</font>\n",
    "\n",
    "**Share your Kaggle profile link  with us here to score , points in  the competition.**\n",
    "\n",
    "**For full points, you need a minimum accuracy of `75%` on the test data. If accuracy is less than `70%`, you gain  no points for this section.**\n",
    "\n",
    "\n",
    "**Submit `submission.csv` (prediction for images in `test.csv`), in the `Submit Predictions` tab in Kaggle, to get evaluated for  this section.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10665762,
     "sourceId": 90936,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "opencv_pytorch (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
